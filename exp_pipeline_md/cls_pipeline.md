# Classification Model Evaluation Pipeline (Final Version)

## 0. Purpose of This Document
This document defines the **final and complete experimental design + analysis pipeline** for the **classification stage** of the COMP9417 project.

It covers **only the classification evaluation phase** — not EDA, preprocessing, anomaly detection, nor feature engineering. Those steps are assumed to be completed beforehand.

This file describes:
- The **four experimental dimensions** to be evaluated
- The **experimental matrix** actually executed
- How results are collected
- How the final analysis is generated
- What outputs are required for the report

This pipeline ensures that experiments are **analysis-driven**, not brute-force enumerated. It keeps the evaluation clean, interpretable, and aligned with project requirements.

---

# 1. Core Experimental Dimensions
Classification experiments evaluate performance along **four orthogonal dimensions**:

## **Dimension 1 — Model Family (RQ1)**
**Goal:** Determine which model class performs best on this task.

Models evaluated:
- **Naive baseline** (predict current class c_t → y_{t+h})
- **Logistic Regression (L2)**
- **Random Forest**
- **XGBoost**  ← chosen over Gradient Boosting because:
  - better for tabular data
  - faster
  - stronger performance
  - clearer feature importance

This answers:
> *Do nonlinear models outperform linear ones and the naive baseline?*

---

## **Dimension 2 — Anomaly Detection Effect (RQ2)**
**Goal:** Evaluate whether anomaly removal improves predictive performance.

We compare:
- **orig** FE datasets (no anomaly cleaning)
- **cleaned** FE datasets (after enhanced consensus anomaly detection)

Under identical settings:
- Same FE (hourly/daily/merge)
- Same models
- Same horizons
- Same test set

This answers:
> *Does cleaning help or hurt classification accuracy?*

---

## **Dimension 3 — Feature Engineering Effect (RQ3)**
**Goal:** Determine which temporal feature design is most effective.

We evaluate three FE granularities:
- **Hourly FE** — fine‑grained temporal dynamics
- **Daily FE** — coarse aggregated patterns
- **Merge FE** — multi‑scale fusion (hourly + daily)

This answers:
> *Which temporal representation provides the most predictive information?*

---

## **Dimension 4 — Forecast Horizon Effect (Implicit Dimension)**
Horizons:
- **h ∈ {1, 6, 12, 24}** (hours ahead)

This answers:
> *How does prediction performance degrade as we forecast further into the future?*

---

# 2. Final Experiment Matrix (Executed Once)
We **run all models × all FE × orig/cleaned × all horizons** once and save the full result table (`df_res`).

We do this because:
- It is efficient
- It avoids manually rerunning subsets
- It allows flexible post‑hoc slicing for each research question

**Then**, the analysis stage extracts the subsets required to answer each RQ.

This isolates the four evaluation dimensions **at the analysis level**, not at the execution level.

---

# 3. Data Inputs Required
From prior steps, we use:

### **Train (2004)**
- `train_2004_fe_hourly_orig.csv`
- `train_2004_fe_daily_orig.csv`
- `train_2004_fe_merge_orig.csv`
- `train_2004_fe_hourly_cleaned.csv`
- `train_2004_fe_daily_cleaned.csv`
- `train_2004_fe_merge_cleaned.csv`

### **Test (2005)**
- `test_2005_fe_hourly.csv`
- `test_2005_fe_daily.csv`
- `test_2005_fe_merge.csv`

These are generated by the feature engineering pipeline.

---

# 4. Output File Structure
After running the evaluation notebook, the outputs are organized as:

```
cls_results/
    summary_full_run.csv          # all experiment results
    figs/
        cm_*.png                  # confusion matrices
        model_comparison_*.png    # RQ1
        cleaning_effect_*.png     # RQ2
        fe_effect_*.png           # RQ3
        horizon_effect_*.png      # RQ4
    analysis/
        rq1_model_comparison.csv
        rq2_anomaly_effect.csv
        rq3_fe_effect.csv
        rq4_horizon_effect.csv
```

Everything needed for the **final report** comes from these files.

---

# 5. Analysis Breakdown by Dimension
After full results are computed, the notebook runs **four isolated analysis modules**, each producing its own tables and plots.

## **RQ1 — Model Comparison Analysis**
From `df_res`, filter:
- FE = merge
- dataset = cleaned
- compare model performance across horizons

Outputs:
- best model per horizon table
- accuracy/Macro‑F1/Macro‑Recall curves
- confusion matrices

---

## **RQ2 — Anomaly Detection Analysis**
For each FE setting (hourly/daily/merge):
- compare `orig` vs `cleaned`
- compute Δ(F1_cleaned − F1_orig)

Outputs:
- anomaly detection uplift table
- bar chart of ΔF1
- removal statistics

---

## **RQ3 — Feature Engineering Analysis**
Using **cleaned data only**:
- compare hourly vs daily vs merge
- analyze stability across horizons

Outputs:
- FE performance heatmap
- FE vs horizon curve plot
- FE ranking table

---

## **RQ4 — Horizon Analysis**
Across all FE and all models:
- group by horizon
- compute mean metrics
- show performance decay curve

Outputs:
- accuracy vs horizon
- Macro‑F1 vs horizon
- robustness comparison

---

# 6. Confusion Matrix Style (Academic)
All CMs use:
- 3×3 grid
- axis labels: Low / Medium / High
- color map: Blues
- value annotations
- consistent format for report quality

---

# 7. Interpretation Strategy (For Report Writing)
Use the analyses to answer the research questions:

### **Models (RQ1)**
- Non-linear models (RF, XGBoost) outperform LR → data has strong nonlinear temporal relationships.
- XGBoost provides highest overall scores → chosen as final model.

### **Anomaly Detection (RQ2)**
- If cleaning improves performance → anomalies were harmful noise.
- If cleaning reduces performance → anomalies capture rare but informative patterns.
Either outcome gives **valuable insight**.

### **Feature Engineering (RQ3)**
- Merge > hourly > daily → multi-scale temporal context improves predictions.

### **Horizon (RQ4)**
- Clear performance decay as horizon increases.
- Some models degrade slower → more robust for long-term forecasting.

---

# 8. Summary
This final pipeline:
- Runs the full experiment grid **once**
- Extracts each research dimension in isolated analysis modules
- Produces organized tables and publication-quality plots
- Fully satisfies COMP9417 project requirements
- Eliminates unnecessary experiment combinations
- Ensures clarity and interpretability

If you want, I can now produce:
- the **full evaluation Jupyter notebook** implementing this pipeline
- the **plot templates**
- the **report writing paragraphs**
Just tell me and I will generate the next stage.

