{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aa942b8-a455-4e68-bf64-0ec763ece66b",
   "metadata": {},
   "source": [
    "# 6. Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b9091-597b-441c-a525-ab4ebfe32779",
   "metadata": {},
   "source": [
    "### Cell 1 — Overview, imports, global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272ef97e-893c-4927-b697-12e312d753bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLASSIFICATION MODEL EVALUATION – FINAL EXPERIMENT PIPELINE\n",
      "======================================================================\n",
      "\n",
      "Experiment configuration:\n",
      "Horizons: [1, 6, 12, 24]\n",
      "FE types: ['hourly', 'daily', 'merge']\n",
      "Data vers: ['orig', 'cleaned']\n",
      "Results dir: ../results_cls\n",
      "Run tag: 20251119_014125\n",
      "\n",
      "[STEP 0] Loading base data for label construction...\n",
      "base_df shape: (8833, 12)\n",
      "base_df range: 2004-03-10 18:00:00 → 2005-04-04 14:00:00\n",
      "Years present: [2004, 2005]\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Cell 1: Imports & global configuration\n",
    "# ================================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "# Optional XGBoost; if not available, fall back to GradientBoosting\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CLASSIFICATION MODEL EVALUATION – FINAL EXPERIMENT PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ---------------- Paths & configuration ----------------\n",
    "PROJECT_ROOT = Path(\"..\")\n",
    "\n",
    "PREP_DIR = PROJECT_ROOT / \"output_Preprocessing_TemporalDataSplitting\"\n",
    "FE_DIR   = PROJECT_ROOT / \"output_FeatureEngineering\"\n",
    "\n",
    "FE_TRAIN_ORIG_DIR  = FE_DIR / \"train\" / \"orig\"\n",
    "FE_TRAIN_CLEAN_DIR = FE_DIR / \"train\" / \"cleaned\"\n",
    "FE_TEST_DIR        = FE_DIR / \"test\"\n",
    "\n",
    "RESULTS_DIR  = PROJECT_ROOT / \"results_cls\"\n",
    "FIG_DIR      = RESULTS_DIR / \"figs\"\n",
    "ANALYSIS_DIR = RESULTS_DIR / \"analysis\"\n",
    "\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "HORIZONS    = [1, 6, 12, 24]\n",
    "FE_TYPES    = [\"hourly\", \"daily\", \"merge\"]\n",
    "DATA_VERS   = [\"orig\", \"cleaned\"]\n",
    "CLASS_NAMES = [\"Low\", \"Medium\", \"High\"]\n",
    "\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(\"\\nExperiment configuration:\")\n",
    "print(f\"Horizons: {HORIZONS}\")\n",
    "print(f\"FE types: {FE_TYPES}\")\n",
    "print(f\"Data vers: {DATA_VERS}\")\n",
    "print(f\"Results dir: {RESULTS_DIR}\")\n",
    "print(f\"Run tag: {RUN_TAG}\")\n",
    "\n",
    "# ---------------- Load base data for labels ----------------\n",
    "print(\"\\n[STEP 0] Loading base data for label construction...\")\n",
    "\n",
    "base_df = pd.read_csv(\n",
    "    PREP_DIR / \"preprocessed_data.csv\",\n",
    "    index_col=\"DateTime\",\n",
    "    parse_dates=True,\n",
    ")\n",
    "base_df = base_df.sort_index()\n",
    "\n",
    "if \"CO(GT)\" not in base_df.columns:\n",
    "    raise KeyError(\"Column 'CO(GT)' not found in preprocessed_data.csv\")\n",
    "\n",
    "print(f\"base_df shape: {base_df.shape}\")\n",
    "print(f\"base_df range: {base_df.index.min()} → {base_df.index.max()}\")\n",
    "print(f\"Years present: {sorted(base_df.index.year.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e8a59-6b13-41ff-9c52-8fe7dc5bca44",
   "metadata": {},
   "source": [
    "### Cell 2 — Helpers: loading data, labels, models, confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95d9f9c-3a5a-4e4f-8520-f3119c172b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Cell 2: Helper functions (labels, loaders, models, CM plotting)\n",
    "# ================================================================\n",
    "\n",
    "# ---------------- Label construction ----------------\n",
    "def make_future_cls_label(df: pd.DataFrame, h: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Build classification labels y_{t+h} from CO(GT) with 3 bins:\n",
    "      0: CO(GT) < 1.5\n",
    "      1: 1.5 ≤ CO(GT) < 2.5\n",
    "      2: CO(GT) ≥ 2.5\n",
    "    \"\"\"\n",
    "    y_future = df[\"CO(GT)\"].shift(-h)\n",
    "    bins = [-np.inf, 1.5, 2.5, np.inf]\n",
    "    y_cls = pd.cut(y_future, bins=bins, labels=[0, 1, 2]).astype(\"Int64\")\n",
    "    y_cls = y_cls.dropna().astype(int)\n",
    "    return y_cls\n",
    "\n",
    "\n",
    "# ---------------- Feature loaders ----------------\n",
    "def load_train_test_fe(fe_type: str, data_ver: str):\n",
    "    \"\"\"\n",
    "    Load train (2004) and test (2005) feature tables for a given\n",
    "    FE type (hourly/daily/merge) and data version (orig/cleaned).\n",
    "    \"\"\"\n",
    "    if fe_type not in FE_TYPES:\n",
    "        raise ValueError(f\"Unknown FE type: {fe_type}\")\n",
    "\n",
    "    if data_ver == \"orig\":\n",
    "        train_path = FE_TRAIN_ORIG_DIR / f\"train_2004_fe_{fe_type}_orig.csv\"\n",
    "    elif data_ver == \"cleaned\":\n",
    "        train_path = FE_TRAIN_CLEAN_DIR / f\"train_2004_fe_{fe_type}_cleaned.csv\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown data version: {data_ver}\")\n",
    "\n",
    "    test_path = FE_TEST_DIR / f\"test_2005_fe_{fe_type}.csv\"\n",
    "\n",
    "    if not train_path.exists():\n",
    "        raise FileNotFoundError(f\"Train FE file not found: {train_path}\")\n",
    "    if not test_path.exists():\n",
    "        raise FileNotFoundError(f\"Test FE file not found: {test_path}\")\n",
    "\n",
    "    X_tr = pd.read_csv(train_path, index_col=\"DateTime\", parse_dates=True)\n",
    "    X_te = pd.read_csv(test_path, index_col=\"DateTime\", parse_dates=True)\n",
    "\n",
    "    X_tr = X_tr.sort_index()\n",
    "    X_te = X_te.sort_index()\n",
    "\n",
    "    return X_tr, X_te\n",
    "\n",
    "\n",
    "def build_train_test_for_fe(fe_type: str, data_ver: str, h: int):\n",
    "    \"\"\"\n",
    "    For a given FE type + data version + horizon, construct aligned\n",
    "    (X_train, y_train, X_test, y_test) with time-respecting splits.\n",
    "    \"\"\"\n",
    "    X_tr, X_te = load_train_test_fe(fe_type, data_ver)\n",
    "    y_full = make_future_cls_label(base_df, h)\n",
    "\n",
    "    # Train: 2004 only, and avoid using labels that look into 2005\n",
    "    idx_tr = X_tr.index.intersection(y_full.index)\n",
    "    boundary = pd.Timestamp(\"2004-12-31 23:00:00\") - pd.Timedelta(hours=h - 1)\n",
    "    idx_tr = idx_tr[idx_tr <= boundary]\n",
    "    X_train = X_tr.loc[idx_tr]\n",
    "    y_train = y_full.loc[idx_tr]\n",
    "\n",
    "    # Test: 2005 only\n",
    "    idx_te = X_te.index.intersection(y_full.index)\n",
    "    X_test = X_te.loc[idx_te]\n",
    "    y_test = y_full.loc[idx_te]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "# ---------------- Model zoo ----------------\n",
    "def model_zoo():\n",
    "    \"\"\"\n",
    "    Return a dict of candidate models:\n",
    "      - Logistic Regression\n",
    "      - Random Forest\n",
    "      - XGBoost (if available) or GradientBoosting as fallback\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "\n",
    "    models[\"LogReg\"] = LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        multi_class=\"auto\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    models[\"RF\"] = RandomForestClassifier(\n",
    "        n_estimators=400,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    if HAS_XGB:\n",
    "        models[\"XGB\"] = XGBClassifier(\n",
    "            n_estimators=400,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            objective=\"multi:softmax\",\n",
    "            num_class=3,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "    else:\n",
    "        print(\"xgboost not available, using GradientBoostingClassifier as GB.\")\n",
    "        models[\"GB\"] = GradientBoostingClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=3,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "# ---------------- Metrics ----------------\n",
    "def eval_metrics(y_true, y_pred) -> dict:\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Macro-F1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Macro-Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------- Confusion matrix plotting ----------------\n",
    "def save_confusion(y_true, y_pred, title: str, path_png: Path):\n",
    "    \"\"\"\n",
    "    Save a 3×3 confusion matrix figure with labels:\n",
    "        Low / Medium / High\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2])\n",
    "\n",
    "    plt.figure(figsize=(4.5, 4.0))\n",
    "    im = plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "    plt.title(title, fontsize=10)\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "\n",
    "    tick_marks = np.arange(len(CLASS_NAMES))\n",
    "    plt.xticks(tick_marks, CLASS_NAMES, rotation=45, ha=\"right\")\n",
    "    plt.yticks(tick_marks, CLASS_NAMES)\n",
    "\n",
    "    thresh = cm.max() / 2.0 if cm.max() > 0 else 0.5\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            val = cm[i, j]\n",
    "            plt.text(\n",
    "                j,\n",
    "                i,\n",
    "                str(val),\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=\"white\" if val > thresh else \"black\",\n",
    "                fontsize=9,\n",
    "            )\n",
    "\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_png, dpi=220)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d00485-2af4-4f72-ba18-6d2d0cb8c99d",
   "metadata": {},
   "source": [
    "### Cell 3 — Run full experiment grid, save summary_full_run_*.csv + confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02fb79a1-7be1-43b4-bbf4-5121a17c98ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1] Running full experiment grid...\n",
      "\n",
      "[Naive] Horizon h = 1\n",
      "\n",
      "[Naive] Horizon h = 6\n",
      "\n",
      "[Naive] Horizon h = 12\n",
      "\n",
      "[Naive] Horizon h = 24\n",
      "\n",
      "============================================================\n",
      "[GRID] Horizon h = 1\n",
      "============================================================\n",
      "FE = hourly | data = orig   \n",
      "Train size: 6590, Test size: 2230, Removed by cleaning: 0\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = hourly | data = cleaned\n",
      "Train size: 6414, Test size: 2230, Removed by cleaning: 176\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = daily  | data = orig   \n",
      "Train size: 252, Test size: 93, Removed by cleaning: 0\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = daily  | data = cleaned\n",
      "Train size: 252, Test size: 93, Removed by cleaning: 0\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = merge  | data = orig   \n",
      "Train size: 6006, Test size: 2230, Removed by cleaning: 0\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = merge  | data = cleaned\n",
      "Train size: 5845, Test size: 2230, Removed by cleaning: 161\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "\n",
      "============================================================\n",
      "[GRID] Horizon h = 6\n",
      "============================================================\n",
      "FE = hourly | data = orig   \n",
      "Train size: 6585, Test size: 2225, Removed by cleaning: 0\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = hourly | data = cleaned\n",
      "Train size: 6409, Test size: 2225, Removed by cleaning: 176\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = daily  | data = orig   \n",
      "Train size: 252, Test size: 93, Removed by cleaning: 0\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = daily  | data = cleaned\n",
      "Train size: 252, Test size: 93, Removed by cleaning: 0\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = merge  | data = orig   \n",
      "Train size: 6001, Test size: 2225, Removed by cleaning: 0\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = merge  | data = cleaned\n",
      "Train size: 5840, Test size: 2225, Removed by cleaning: 161\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "\n",
      "============================================================\n",
      "[GRID] Horizon h = 12\n",
      "============================================================\n",
      "FE = hourly | data = orig   \n",
      "Train size: 6585, Test size: 2219, Removed by cleaning: 0\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = hourly | data = cleaned\n",
      "Train size: 6409, Test size: 2219, Removed by cleaning: 176\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = daily  | data = orig   \n",
      "Train size: 252, Test size: 93, Removed by cleaning: 0\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = daily  | data = cleaned\n",
      "Train size: 252, Test size: 93, Removed by cleaning: 0\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = merge  | data = orig   \n",
      "Train size: 6001, Test size: 2219, Removed by cleaning: 0\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = merge  | data = cleaned\n",
      "Train size: 5840, Test size: 2219, Removed by cleaning: 161\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "\n",
      "============================================================\n",
      "[GRID] Horizon h = 24\n",
      "============================================================\n",
      "FE = hourly | data = orig   \n",
      "Train size: 6585, Test size: 2207, Removed by cleaning: 0\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = hourly | data = cleaned\n",
      "Train size: 6409, Test size: 2207, Removed by cleaning: 176\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = daily  | data = orig   \n",
      "Train size: 252, Test size: 92, Removed by cleaning: 0\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = daily  | data = cleaned\n",
      "Train size: 252, Test size: 92, Removed by cleaning: 0\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = merge  | data = orig   \n",
      "Train size: 6001, Test size: 2207, Removed by cleaning: 0\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "FE = merge  | data = cleaned\n",
      "Train size: 5840, Test size: 2207, Removed by cleaning: 161\n",
      "  Model: LogReg\n",
      "  Model: RF\n",
      "  Model: XGB\n",
      "\n",
      "Full experiment grid finished.\n",
      "Summary saved to: ../results_cls/summary_full_run_20251119_014125.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>fe_type</th>\n",
       "      <th>data_ver</th>\n",
       "      <th>model</th>\n",
       "      <th>train_n</th>\n",
       "      <th>train_n_eff</th>\n",
       "      <th>clean_removed</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-F1</th>\n",
       "      <th>Macro-Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>daily</td>\n",
       "      <td>cleaned</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>252.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>0.515093</td>\n",
       "      <td>0.517472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>daily</td>\n",
       "      <td>orig</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>252.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.688172</td>\n",
       "      <td>0.583563</td>\n",
       "      <td>0.582030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>hourly</td>\n",
       "      <td>cleaned</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>6414.0</td>\n",
       "      <td>6414.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>0.760538</td>\n",
       "      <td>0.728104</td>\n",
       "      <td>0.737527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>hourly</td>\n",
       "      <td>orig</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>6590.0</td>\n",
       "      <td>6590.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.767265</td>\n",
       "      <td>0.739668</td>\n",
       "      <td>0.748551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>merge</td>\n",
       "      <td>cleaned</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>5845.0</td>\n",
       "      <td>5845.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.742152</td>\n",
       "      <td>0.736794</td>\n",
       "      <td>0.751057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    h fe_type data_ver   model  train_n  train_n_eff  clean_removed  Accuracy  \\\n",
       "13  1   daily  cleaned  LogReg    252.0        252.0            0.0  0.677419   \n",
       "10  1   daily     orig  LogReg    252.0        252.0            0.0  0.688172   \n",
       "7   1  hourly  cleaned  LogReg   6414.0       6414.0          176.0  0.760538   \n",
       "4   1  hourly     orig  LogReg   6590.0       6590.0            0.0  0.767265   \n",
       "19  1   merge  cleaned  LogReg   5845.0       5845.0          161.0  0.742152   \n",
       "\n",
       "    Macro-F1  Macro-Recall  \n",
       "13  0.515093      0.517472  \n",
       "10  0.583563      0.582030  \n",
       "7   0.728104      0.737527  \n",
       "4   0.739668      0.748551  \n",
       "19  0.736794      0.751057  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Cell 3: Run full experiment grid\n",
    "# ================================================================\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\n[STEP 1] Running full experiment grid...\")\n",
    "\n",
    "# ---------------- Naive baseline ----------------\n",
    "def eval_naive_baseline(h: int):\n",
    "    \"\"\"\n",
    "    Naive baseline:\n",
    "      y_{t+h} = class of CO(GT) at t+h\n",
    "      y_hat   = class of CO(GT) at t\n",
    "    \"\"\"\n",
    "    y = make_future_cls_label(base_df, h)\n",
    "\n",
    "    bins = [-np.inf, 1.5, 2.5, np.inf]\n",
    "    c_t_cls = pd.cut(base_df[\"CO(GT)\"], bins=bins, labels=[0, 1, 2]).astype(\"Int64\")\n",
    "    c_t_cls = c_t_cls.dropna().astype(int)\n",
    "\n",
    "    idx = y.index.intersection(c_t_cls.index)\n",
    "\n",
    "    y = y.loc[idx]\n",
    "    yhat = c_t_cls.loc[idx]\n",
    "\n",
    "    boundary = pd.Timestamp(\"2004-12-31 23:00:00\") - pd.Timedelta(hours=h - 1)\n",
    "    train_idx = idx[(idx.year == 2004) & (idx <= boundary)]\n",
    "    test_idx = idx[idx.year == 2005]\n",
    "\n",
    "    y_te = y.loc[test_idx]\n",
    "    yhat_te = yhat.loc[test_idx]\n",
    "\n",
    "    metrics = eval_metrics(y_te, yhat_te)\n",
    "    return metrics, y_te, yhat_te\n",
    "\n",
    "\n",
    "# 1) Naive baseline per horizon\n",
    "for h in HORIZONS:\n",
    "    print(f\"\\n[Naive] Horizon h = {h}\")\n",
    "    naive_metrics, y_te_naive, yhat_naive = eval_naive_baseline(h)\n",
    "\n",
    "    results.append({\n",
    "        \"h\": h,\n",
    "        \"fe_type\": \"N/A\",\n",
    "        \"data_ver\": \"N/A\",\n",
    "        \"model\": \"Naive\",\n",
    "        \"train_n\": np.nan,\n",
    "        \"train_n_eff\": np.nan,\n",
    "        \"clean_removed\": np.nan,\n",
    "        **naive_metrics,\n",
    "    })\n",
    "\n",
    "    cm_path = FIG_DIR / f\"cm_naive_h{h}.png\"\n",
    "    save_confusion(y_te_naive, yhat_naive, f\"Naive baseline (h={h})\", cm_path)\n",
    "\n",
    "\n",
    "# 2) Models with feature sets\n",
    "model_dict = model_zoo()\n",
    "\n",
    "for h in HORIZONS:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"[GRID] Horizon h = {h}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for fe_type in FE_TYPES:\n",
    "        for data_ver in DATA_VERS:\n",
    "            print(f\"FE = {fe_type:6s} | data = {data_ver:7s}\")\n",
    "\n",
    "            X_tr, y_tr, X_te, y_te = build_train_test_for_fe(fe_type, data_ver, h)\n",
    "            n_train = len(X_tr)\n",
    "\n",
    "            if data_ver == \"orig\":\n",
    "                clean_removed = 0\n",
    "            else:\n",
    "                X_tr_orig, _, _, _ = build_train_test_for_fe(fe_type, \"orig\", h)\n",
    "                clean_removed = max(0, len(X_tr_orig) - len(X_tr))\n",
    "\n",
    "            print(f\"Train size: {len(X_tr)}, Test size: {len(X_te)}, Removed by cleaning: {clean_removed}\")\n",
    "\n",
    "            scaler = StandardScaler().fit(X_tr)\n",
    "            X_tr_s = scaler.transform(X_tr)\n",
    "            X_te_s = scaler.transform(X_te)\n",
    "\n",
    "            for m_name, proto in model_dict.items():\n",
    "                print(f\"  Model: {m_name}\")\n",
    "\n",
    "                model_cls = proto.__class__\n",
    "                model = model_cls(**proto.get_params())\n",
    "\n",
    "                model.fit(X_tr_s, y_tr)\n",
    "                yhat = model.predict(X_te_s)\n",
    "                metrics = eval_metrics(y_te, yhat)\n",
    "\n",
    "                results.append({\n",
    "                    \"h\": h,\n",
    "                    \"fe_type\": fe_type,\n",
    "                    \"data_ver\": data_ver,\n",
    "                    \"model\": m_name,\n",
    "                    \"train_n\": int(n_train),\n",
    "                    \"train_n_eff\": int(len(X_tr)),\n",
    "                    \"clean_removed\": int(clean_removed),\n",
    "                    **metrics,\n",
    "                })\n",
    "\n",
    "                cm_title = f\"{m_name} | FE={fe_type} | {data_ver} | h={h}\"\n",
    "                safe_data_ver = data_ver.replace(\"(\", \"\").replace(\")\", \"\").replace(\" \", \"_\")\n",
    "                cm_path = FIG_DIR / f\"cm_{m_name}_FE-{fe_type}_{safe_data_ver}_h{h}.png\"\n",
    "                save_confusion(y_te, yhat, cm_title, cm_path)\n",
    "\n",
    "# 3) Save summary\n",
    "df_res = pd.DataFrame(results)\n",
    "df_res = df_res[\n",
    "    [\"h\", \"fe_type\", \"data_ver\", \"model\", \"train_n\", \"train_n_eff\",\n",
    "     \"clean_removed\", \"Accuracy\", \"Macro-F1\", \"Macro-Recall\"]\n",
    "].sort_values([\"h\", \"model\", \"fe_type\", \"data_ver\"])\n",
    "\n",
    "summary_path = RESULTS_DIR / f\"summary_full_run_{RUN_TAG}.csv\"\n",
    "df_res.to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"\\nFull experiment grid finished.\")\n",
    "print(f\"Summary saved to: {summary_path}\")\n",
    "df_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f05db55-aba9-4250-b9e3-b82b4ea2f745",
   "metadata": {},
   "source": [
    "### Cell 4 — Analysis by four dimensions (RQ1–RQ4) + plots + CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe6ba521-9a34-470b-aa8c-5aabe5427e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] Post-hoc analysis for 4 evaluation dimensions...\n",
      "\n",
      "[RQ1] Model comparison (cleaned data, FE = merge)...\n",
      "Saved RQ1 summary to: ../results_cls/analysis/rq1_model_comparison.csv\n",
      "\n",
      "[RQ2] Effect of anomaly cleaning (orig vs cleaned)...\n",
      "Saved RQ2 delta table to: ../results_cls/analysis/rq2_anomaly_effect.csv\n",
      "\n",
      "[RQ3] Feature engineering comparison (cleaned data only)...\n",
      "Saved RQ3 summary to: ../results_cls/analysis/rq3_fe_effect.csv\n",
      "\n",
      "[RQ4] Horizon effect (average over models/FE/versions)...\n",
      "Saved RQ4 summary to: ../results_cls/analysis/rq4_horizon_effect.csv\n",
      "\n",
      "Analysis complete.\n",
      "Full grid results: ../results_cls/summary_full_run_20251119_014125.csv\n",
      "Analysis CSVs    : ../results_cls/analysis\n",
      "Figures          : ../results_cls/figs\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Cell 4: Post-hoc analysis for RQ1–RQ4\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n[STEP 2] Post-hoc analysis for 4 evaluation dimensions...\")\n",
    "\n",
    "df_all    = df_res.copy()\n",
    "df_naive  = df_all[df_all[\"model\"] == \"Naive\"].copy()\n",
    "df_models = df_all[df_all[\"model\"] != \"Naive\"].copy()\n",
    "\n",
    "PRIMARY = \"Macro-F1\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# RQ1: Model family comparison (cleaned + merge only)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n[RQ1] Model comparison (cleaned data, FE = merge)...\")\n",
    "\n",
    "rq1 = df_models[\n",
    "    (df_models[\"data_ver\"] == \"cleaned\") &\n",
    "    (df_models[\"fe_type\"] == \"merge\")\n",
    "].copy()\n",
    "\n",
    "rq1_summary = (\n",
    "    rq1.groupby([\"h\", \"model\"])[[\"Accuracy\", \"Macro-F1\", \"Macro-Recall\"]]\n",
    "       .mean().reset_index()\n",
    ")\n",
    "\n",
    "rq1_path = ANALYSIS_DIR / \"rq1_model_comparison.csv\"\n",
    "rq1_summary.to_csv(rq1_path, index=False)\n",
    "print(f\"Saved RQ1 summary to: {rq1_path}\")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "for m in sorted(rq1[\"model\"].unique()):\n",
    "    sub = rq1_summary[rq1_summary[\"model\"] == m]\n",
    "    plt.plot(sub[\"h\"], sub[PRIMARY], marker=\"o\", label=m)\n",
    "\n",
    "plt.xlabel(\"Horizon h (hours)\")\n",
    "plt.ylabel(PRIMARY)\n",
    "plt.title(f\"RQ1 – Model comparison (cleaned, FE=merge, metric={PRIMARY})\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"rq1_model_comparison.png\", dpi=220)\n",
    "plt.close()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# RQ2: Anomaly detection effect (orig vs cleaned)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n[RQ2] Effect of anomaly cleaning (orig vs cleaned)...\")\n",
    "\n",
    "def compute_clean_delta(group):\n",
    "    if set(group[\"data_ver\"]) >= {\"orig\", \"cleaned\"}:\n",
    "        m_clean = group[group[\"data_ver\"] == \"cleaned\"][[\"Accuracy\", \"Macro-F1\", \"Macro-Recall\"]].mean()\n",
    "        m_orig  = group[group[\"data_ver\"] == \"orig\"][[\"Accuracy\", \"Macro-F1\", \"Macro-Recall\"]].mean()\n",
    "        return (m_clean - m_orig)\n",
    "    else:\n",
    "        return pd.Series({\"Accuracy\": np.nan, \"Macro-F1\": np.nan, \"Macro-Recall\": np.nan})\n",
    "\n",
    "rq2_delta = (\n",
    "    df_models.groupby([\"h\", \"fe_type\", \"model\"]).apply(compute_clean_delta)\n",
    "             .reset_index()\n",
    ")\n",
    "\n",
    "rq2_path = ANALYSIS_DIR / \"rq2_anomaly_effect.csv\"\n",
    "rq2_delta.to_csv(rq2_path, index=False)\n",
    "print(f\"Saved RQ2 delta table to: {rq2_path}\")\n",
    "\n",
    "rq2_fe_summary = (\n",
    "    rq2_delta.groupby(\"fe_type\")[[\"Accuracy\", \"Macro-F1\", \"Macro-Recall\"]]\n",
    "             .mean().reset_index()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.bar(rq2_fe_summary[\"fe_type\"], rq2_fe_summary[\"Macro-F1\"])\n",
    "plt.xlabel(\"FE type\")\n",
    "plt.ylabel(\"Δ Macro-F1 (cleaned - orig)\")\n",
    "plt.title(\"RQ2 – Average cleaning uplift by FE type\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"rq2_cleaning_uplift_by_fe.png\", dpi=220)\n",
    "plt.close()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# RQ3: Feature engineering effect (cleaned data only)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n[RQ3] Feature engineering comparison (cleaned data only)...\")\n",
    "\n",
    "rq3 = df_models[df_models[\"data_ver\"] == \"cleaned\"].copy()\n",
    "\n",
    "rq3_summary = (\n",
    "    rq3.groupby([\"h\", \"fe_type\"])[[\"Accuracy\", \"Macro-F1\", \"Macro-Recall\"]]\n",
    "       .mean().reset_index()\n",
    ")\n",
    "\n",
    "rq3_path = ANALYSIS_DIR / \"rq3_fe_effect.csv\"\n",
    "rq3_summary.to_csv(rq3_path, index=False)\n",
    "print(f\"Saved RQ3 summary to: {rq3_path}\")\n",
    "\n",
    "pivot = rq3_summary.pivot(index=\"fe_type\", columns=\"h\", values=PRIMARY)\n",
    "\n",
    "plt.figure(figsize=(6, 3.5))\n",
    "im = plt.imshow(pivot.values, aspect=\"auto\", cmap=\"viridis\")\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04, label=PRIMARY)\n",
    "plt.xticks(range(len(pivot.columns)), pivot.columns)\n",
    "plt.yticks(range(len(pivot.index)), pivot.index)\n",
    "plt.xlabel(\"Horizon h (hours)\")\n",
    "plt.ylabel(\"FE type\")\n",
    "plt.title(f\"RQ3 – Feature engineering effect ({PRIMARY})\")\n",
    "\n",
    "for i in range(pivot.shape[0]):\n",
    "    for j in range(pivot.shape[1]):\n",
    "        val = pivot.values[i, j]\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            f\"{val:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"white\" if val < pivot.values.max() / 2 else \"black\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"rq3_fe_effect_heatmap.png\", dpi=220)\n",
    "plt.close()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# RQ4: Horizon effect (performance decay vs h)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n[RQ4] Horizon effect (average over models/FE/versions)...\")\n",
    "\n",
    "rq4_summary = (\n",
    "    df_models.groupby(\"h\")[[\"Accuracy\", \"Macro-F1\", \"Macro-Recall\"]]\n",
    "             .mean().reset_index()\n",
    ")\n",
    "\n",
    "rq4_path = ANALYSIS_DIR / \"rq4_horizon_effect.csv\"\n",
    "rq4_summary.to_csv(rq4_path, index=False)\n",
    "print(f\"Saved RQ4 summary to: {rq4_path}\")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(rq4_summary[\"h\"], rq4_summary[\"Accuracy\"], marker=\"o\", label=\"Accuracy\")\n",
    "plt.plot(rq4_summary[\"h\"], rq4_summary[\"Macro-F1\"], marker=\"o\", label=\"Macro-F1\")\n",
    "plt.plot(rq4_summary[\"h\"], rq4_summary[\"Macro-Recall\"], marker=\"o\", label=\"Macro-Recall\")\n",
    "plt.xlabel(\"Horizon h (hours)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"RQ4 – Average performance vs horizon\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"rq4_perf_vs_horizon.png\", dpi=220)\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nAnalysis complete.\")\n",
    "print(f\"Full grid results: {summary_path}\")\n",
    "print(f\"Analysis CSVs    : {ANALYSIS_DIR}\")\n",
    "print(f\"Figures          : {FIG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6bdf6d-5b60-4b30-b1e8-9009ba83e110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
