{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae89a6c-cbe8-4722-ab1f-077b0cd415e5",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b7fd11-67dd-4011-a4c6-af8327cd71eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE ENGINEERING FOR 2004 (TRAIN) AND 2005 (TEST)\n",
      "======================================================================\n",
      "Output directory: ../output_FeatureEngineering\n",
      "\n",
      "[STEP 1] Loading preprocessed and cleaned data\n",
      "train_2004_orig shape: (6602, 12)\n",
      "train_2004_cleaned shape: (6426, 12)\n",
      "test_2005 shape: (2231, 12)\n",
      "train_2004_orig range: 2004-03-10 18:00:00 → 2004-12-31 23:00:00\n",
      "train_2004_cleaned range: 2004-03-10 18:00:00 → 2004-12-31 23:00:00\n",
      "test_2005 range: 2005-01-01 00:00:00 → 2005-04-04 14:00:00\n",
      "\n",
      "[INFO] Combined pipelines\n",
      "df_all_orig shape: (8833, 12) range: 2004-03-10 18:00:00 → 2005-04-04 14:00:00\n",
      "df_all_cleaned shape: (8657, 12) range: 2004-03-10 18:00:00 → 2005-04-04 14:00:00\n",
      "\n",
      "------------------------------------------------------------\n",
      "[BUILD FEATURES] version = orig_all\n",
      "Input shape: (8833, 12)\n",
      "Date range: 2004-03-10 18:00:00 → 2005-04-04 14:00:00\n",
      "[Step 2] Creating hourly features\n",
      "Adding hourly lags (1, 3, 6, 12 h)\n",
      "Adding moving averages (4, 8, 12 h)\n",
      "Adding rate-of-change features\n",
      "Hourly feature table shape: (8821, 56)\n",
      "[Step 3] Creating daily features\n",
      "Adding daily lags (1, 2, 7 d)\n",
      "Daily feature table shape: (350, 51)\n",
      "[Step 4] Creating merged hourly + daily features\n",
      "Merged feature table shape: (8237, 104)\n",
      "Number of daily features merged per row: 48\n",
      "\n",
      "[OUTPUT] Original pipeline (train 2004)\n",
      "../output_FeatureEngineering/train/orig/train_2004_fe_hourly_orig.csv\n",
      "../output_FeatureEngineering/train/orig/train_2004_fe_daily_orig.csv\n",
      "../output_FeatureEngineering/train/orig/train_2004_fe_merge_orig.csv\n",
      "\n",
      "[OUTPUT] Shared test pipeline (2005)\n",
      "../output_FeatureEngineering/test/test_2005_fe_hourly.csv\n",
      "../output_FeatureEngineering/test/test_2005_fe_daily.csv\n",
      "../output_FeatureEngineering/test/test_2005_fe_merge.csv\n",
      "\n",
      "------------------------------------------------------------\n",
      "[BUILD FEATURES] version = cleaned_all\n",
      "Input shape: (8657, 12)\n",
      "Date range: 2004-03-10 18:00:00 → 2005-04-04 14:00:00\n",
      "[Step 2] Creating hourly features\n",
      "Adding hourly lags (1, 3, 6, 12 h)\n",
      "Adding moving averages (4, 8, 12 h)\n",
      "Adding rate-of-change features\n",
      "Hourly feature table shape: (8645, 56)\n",
      "[Step 3] Creating daily features\n",
      "Adding daily lags (1, 2, 7 d)\n",
      "Daily feature table shape: (350, 51)\n",
      "[Step 4] Creating merged hourly + daily features\n",
      "Merged feature table shape: (8076, 104)\n",
      "Number of daily features merged per row: 48\n",
      "\n",
      "[OUTPUT] Cleaned pipeline (train 2004 after anomaly removal)\n",
      "../output_FeatureEngineering/train/cleaned/train_2004_fe_hourly_cleaned.csv\n",
      "../output_FeatureEngineering/train/cleaned/train_2004_fe_daily_cleaned.csv\n",
      "../output_FeatureEngineering/train/cleaned/train_2004_fe_merge_cleaned.csv\n",
      "\n",
      "[SUMMARY]\n",
      "                version  n_rows  n_features\n",
      "0     train_orig_hourly    6590          56\n",
      "1      train_orig_daily     256          51\n",
      "2      train_orig_merge    6006         104\n",
      "3  train_cleaned_hourly    6414          56\n",
      "4   train_cleaned_daily     256          51\n",
      "5   train_cleaned_merge    5845         104\n",
      "6      test_2005_hourly    2231          56\n",
      "7       test_2005_daily      94          51\n",
      "8       test_2005_merge    2231         104\n",
      "\n",
      "Summary saved to: ../output_FeatureEngineering/fe_train_test_summary.csv\n",
      "\n",
      "======================================================================\n",
      "FEATURE ENGINEERING COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Paths and basic setup\n",
    "# --------------------------------------------------\n",
    "PROJECT_ROOT = Path(\"..\")  # from notebook/ back to project root\n",
    "PREP_DIR = PROJECT_ROOT / \"output_Preprocessing_TemporalDataSplitting\"\n",
    "ANOM_DIR = PROJECT_ROOT / \"output_AnomalyDetection\"\n",
    "FE_DIR = PROJECT_ROOT / \"output_FeatureEngineering\"\n",
    "\n",
    "FE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FE_TRAIN_DIR = FE_DIR / \"train\"\n",
    "FE_TRAIN_ORIG_DIR = FE_TRAIN_DIR / \"orig\"\n",
    "FE_TRAIN_CLEAN_DIR = FE_TRAIN_DIR / \"cleaned\"\n",
    "FE_TEST_DIR = FE_DIR / \"test\"\n",
    "\n",
    "FE_TRAIN_ORIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FE_TRAIN_CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FE_TEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE ENGINEERING FOR 2004 (TRAIN) AND 2005 (TEST)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Output directory:\", FE_DIR)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Load preprocessed train/test and cleaned train\n",
    "# --------------------------------------------------\n",
    "print(\"\\n[STEP 1] Loading preprocessed and cleaned data\")\n",
    "\n",
    "train_2004_orig = pd.read_csv(\n",
    "    PREP_DIR / \"train_2004.csv\",\n",
    "    index_col=\"DateTime\",\n",
    "    parse_dates=True,\n",
    ")\n",
    "\n",
    "test_2005 = pd.read_csv(\n",
    "    PREP_DIR / \"test_2005.csv\",\n",
    "    index_col=\"DateTime\",\n",
    "    parse_dates=True,\n",
    ")\n",
    "\n",
    "train_2004_cleaned = pd.read_csv(\n",
    "    ANOM_DIR / \"train_2004_cleaned.csv\",\n",
    "    index_col=\"DateTime\",\n",
    "    parse_dates=True,\n",
    ")\n",
    "\n",
    "train_2004_orig = train_2004_orig.sort_index()\n",
    "test_2005 = test_2005.sort_index()\n",
    "train_2004_cleaned = train_2004_cleaned.sort_index()\n",
    "\n",
    "print(\"train_2004_orig shape:\", train_2004_orig.shape)\n",
    "print(\"train_2004_cleaned shape:\", train_2004_cleaned.shape)\n",
    "print(\"test_2005 shape:\", test_2005.shape)\n",
    "print(\"train_2004_orig range:\", train_2004_orig.index.min(), \"→\", train_2004_orig.index.max())\n",
    "print(\"train_2004_cleaned range:\", train_2004_cleaned.index.min(), \"→\", train_2004_cleaned.index.max())\n",
    "print(\"test_2005 range:\", test_2005.index.min(), \"→\", test_2005.index.max())\n",
    "\n",
    "# Original pipeline: full 2004 + full 2005\n",
    "df_all_orig = pd.concat([train_2004_orig, test_2005]).sort_index()\n",
    "\n",
    "# Cleaned pipeline: cleaned 2004 + full 2005\n",
    "df_all_cleaned = pd.concat([train_2004_cleaned, test_2005]).sort_index()\n",
    "\n",
    "print(\"\\n[INFO] Combined pipelines\")\n",
    "print(\"df_all_orig shape:\", df_all_orig.shape, \"range:\", df_all_orig.index.min(), \"→\", df_all_orig.index.max())\n",
    "print(\"df_all_cleaned shape:\", df_all_cleaned.shape, \"range:\", df_all_cleaned.index.min(), \"→\", df_all_cleaned.index.max())\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Helper functions\n",
    "# --------------------------------------------------\n",
    "def encode_cyclical(df: pd.DataFrame, col: str, max_val: int) -> pd.DataFrame:\n",
    "    df[f\"{col}_sin\"] = np.sin(2 * np.pi * df[col] / max_val)\n",
    "    df[f\"{col}_cos\"] = np.cos(2 * np.pi * df[col] / max_val)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_features_for_df(df: pd.DataFrame, tag: str):\n",
    "    \"\"\"\n",
    "    Build hourly, daily and merged (hourly + daily) feature tables.\n",
    "    df: hourly time series covering 2004 + 2005.\n",
    "    tag: label for logging.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(f\"[BUILD FEATURES] version = {tag}\")\n",
    "    print(\"Input shape:\", df.shape)\n",
    "    print(\"Date range:\", df.index.min(), \"→\", df.index.max())\n",
    "\n",
    "    pollutants = [\"CO(GT)\", \"C6H6(GT)\", \"NOx(GT)\", \"NO2(GT)\"]\n",
    "    sensors = [\n",
    "        \"PT08.S1(CO)\",\n",
    "        \"PT08.S2(NMHC)\",\n",
    "        \"PT08.S3(NOx)\",\n",
    "        \"PT08.S4(NO2)\",\n",
    "        \"PT08.S5(O3)\",\n",
    "    ]\n",
    "    meteo = [\"T\", \"RH\", \"AH\"]\n",
    "\n",
    "    # -----------------------------------\n",
    "    # 2. Hourly features\n",
    "    # -----------------------------------\n",
    "    print(\"[Step 2] Creating hourly features\")\n",
    "\n",
    "    df_hourly = df.copy()\n",
    "\n",
    "    # Calendar indicators from index\n",
    "    df_hourly[\"hour\"] = df_hourly.index.hour\n",
    "    df_hourly[\"day_of_week\"] = df_hourly.index.dayofweek\n",
    "    df_hourly[\"month\"] = df_hourly.index.month\n",
    "    df_hourly[\"is_weekend\"] = (df_hourly[\"day_of_week\"] >= 5).astype(int)\n",
    "\n",
    "    # Sine–cosine encoding for hour and day_of_week\n",
    "    df_hourly = encode_cyclical(df_hourly, \"hour\", 24)\n",
    "    df_hourly = encode_cyclical(df_hourly, \"day_of_week\", 7)\n",
    "\n",
    "    # Short-term lags: 1, 3, 6, 12 hours\n",
    "    print(\"Adding hourly lags (1, 3, 6, 12 h)\")\n",
    "    for pollutant in pollutants:\n",
    "        for lag in [1, 3, 6, 12]:\n",
    "            df_hourly[f\"{pollutant}_lag_{lag}h\"] = df_hourly[pollutant].shift(lag)\n",
    "\n",
    "    # Moving averages: 4, 8, 12 hours\n",
    "    print(\"Adding moving averages (4, 8, 12 h)\")\n",
    "    for pollutant in pollutants:\n",
    "        for window in [4, 8, 12]:\n",
    "            df_hourly[f\"{pollutant}_ma_{window}h\"] = (\n",
    "                df_hourly[pollutant].rolling(window=window, min_periods=1).mean()\n",
    "            )\n",
    "\n",
    "    # Rate-of-change: absolute and percentage\n",
    "    print(\"Adding rate-of-change features\")\n",
    "    for pollutant in pollutants:\n",
    "        df_hourly[f\"{pollutant}_hourly_change\"] = df_hourly[pollutant].diff()\n",
    "        df_hourly[f\"{pollutant}_hourly_pct_change\"] = df_hourly[pollutant].pct_change()\n",
    "\n",
    "    df_hourly_clean = df_hourly.dropna()\n",
    "    print(\"Hourly feature table shape:\", df_hourly_clean.shape)\n",
    "\n",
    "    # -----------------------------------\n",
    "    # 3. Daily features\n",
    "    # -----------------------------------\n",
    "    print(\"[Step 3] Creating daily features\")\n",
    "\n",
    "    daily_agg_dict = {}\n",
    "    for pollutant in pollutants:\n",
    "        daily_agg_dict[pollutant] = [\"mean\", \"max\", \"std\"]\n",
    "    for sensor in sensors:\n",
    "        daily_agg_dict[sensor] = [\"mean\", \"max\", \"std\"]\n",
    "    for meteo_var in meteo:\n",
    "        daily_agg_dict[meteo_var] = [\"mean\", \"max\", \"std\"]\n",
    "\n",
    "    df_daily = df.resample(\"D\").agg(daily_agg_dict)\n",
    "    df_daily.columns = [\"_\".join(col).strip() for col in df_daily.columns.values]\n",
    "\n",
    "    df_daily[\"day_of_week\"] = df_daily.index.dayofweek\n",
    "    df_daily[\"month\"] = df_daily.index.month\n",
    "    df_daily[\"is_weekend\"] = (df_daily[\"day_of_week\"] >= 5).astype(int)\n",
    "\n",
    "    # Daily lags of pollutant means: 1, 2, 7 days\n",
    "    print(\"Adding daily lags (1, 2, 7 d)\")\n",
    "    for col in [\"CO(GT)_mean\", \"C6H6(GT)_mean\", \"NOx(GT)_mean\", \"NO2(GT)_mean\"]:\n",
    "        for lag in [1, 2, 7]:\n",
    "            df_daily[f\"{col}_lag_{lag}d\"] = df_daily[col].shift(lag)\n",
    "\n",
    "    df_daily_clean = df_daily.dropna()\n",
    "    print(\"Daily feature table shape:\", df_daily_clean.shape)\n",
    "\n",
    "    # -----------------------------------\n",
    "    # 4. Merged hourly + daily features\n",
    "    # -----------------------------------\n",
    "    print(\"[Step 4] Creating merged hourly + daily features\")\n",
    "\n",
    "    daily_for_merge = df_daily_clean.copy()\n",
    "    daily_for_merge.index = pd.to_datetime(daily_for_merge.index)\n",
    "\n",
    "    df_full = df_hourly_clean.copy()\n",
    "    df_full[\"merge_date\"] = df_full.index.normalize()\n",
    "\n",
    "    daily_features_to_merge = [\n",
    "        col\n",
    "        for col in daily_for_merge.columns\n",
    "        if col not in [\"day_of_week\", \"month\", \"is_weekend\"]\n",
    "    ]\n",
    "\n",
    "    df_full = df_full.merge(\n",
    "        daily_for_merge[daily_features_to_merge],\n",
    "        left_on=\"merge_date\",\n",
    "        right_index=True,\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_daily\"),\n",
    "    )\n",
    "\n",
    "    df_full = df_full.drop(columns=[\"merge_date\"])\n",
    "    df_full_clean = df_full.dropna()\n",
    "\n",
    "    print(\"Merged feature table shape:\", df_full_clean.shape)\n",
    "    print(\"Number of daily features merged per row:\", len(daily_features_to_merge))\n",
    "\n",
    "    return df_hourly_clean, df_daily_clean, df_full_clean\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Feature engineering for original pipeline\n",
    "# --------------------------------------------------\n",
    "hourly_orig_all, daily_orig_all, merge_orig_all = build_features_for_df(\n",
    "    df_all_orig,\n",
    "    tag=\"orig_all\",\n",
    ")\n",
    "\n",
    "mask_2004_h = hourly_orig_all.index.year == 2004\n",
    "mask_2005_h = hourly_orig_all.index.year == 2005\n",
    "\n",
    "mask_2004_d = daily_orig_all.index.year == 2004\n",
    "mask_2005_d = daily_orig_all.index.year == 2005\n",
    "\n",
    "mask_2004_m = merge_orig_all.index.year == 2004\n",
    "mask_2005_m = merge_orig_all.index.year == 2005\n",
    "\n",
    "train_hourly_orig = hourly_orig_all.loc[mask_2004_h]\n",
    "test_hourly = hourly_orig_all.loc[mask_2005_h]\n",
    "\n",
    "train_daily_orig = daily_orig_all.loc[mask_2004_d]\n",
    "test_daily = daily_orig_all.loc[mask_2005_d]\n",
    "\n",
    "train_merge_orig = merge_orig_all.loc[mask_2004_m]\n",
    "test_merge = merge_orig_all.loc[mask_2005_m]\n",
    "\n",
    "# Save original train features\n",
    "train_hourly_orig.to_csv(FE_TRAIN_ORIG_DIR / \"train_2004_fe_hourly_orig.csv\")\n",
    "train_daily_orig.to_csv(FE_TRAIN_ORIG_DIR / \"train_2004_fe_daily_orig.csv\")\n",
    "train_merge_orig.to_csv(FE_TRAIN_ORIG_DIR / \"train_2004_fe_merge_orig.csv\")\n",
    "\n",
    "print(\"\\n[OUTPUT] Original pipeline (train 2004)\")\n",
    "print(FE_TRAIN_ORIG_DIR / \"train_2004_fe_hourly_orig.csv\")\n",
    "print(FE_TRAIN_ORIG_DIR / \"train_2004_fe_daily_orig.csv\")\n",
    "print(FE_TRAIN_ORIG_DIR / \"train_2004_fe_merge_orig.csv\")\n",
    "\n",
    "# Save shared test features\n",
    "test_hourly.to_csv(FE_TEST_DIR / \"test_2005_fe_hourly.csv\")\n",
    "test_daily.to_csv(FE_TEST_DIR / \"test_2005_fe_daily.csv\")\n",
    "test_merge.to_csv(FE_TEST_DIR / \"test_2005_fe_merge.csv\")\n",
    "\n",
    "print(\"\\n[OUTPUT] Shared test pipeline (2005)\")\n",
    "print(FE_TEST_DIR / \"test_2005_fe_hourly.csv\")\n",
    "print(FE_TEST_DIR / \"test_2005_fe_daily.csv\")\n",
    "print(FE_TEST_DIR / \"test_2005_fe_merge.csv\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Feature engineering for cleaned pipeline\n",
    "# --------------------------------------------------\n",
    "hourly_clean_all, daily_clean_all, merge_clean_all = build_features_for_df(\n",
    "    df_all_cleaned,\n",
    "    tag=\"cleaned_all\",\n",
    ")\n",
    "\n",
    "mask_2004_h_c = hourly_clean_all.index.year == 2004\n",
    "mask_2004_d_c = daily_clean_all.index.year == 2004\n",
    "mask_2004_m_c = merge_clean_all.index.year == 2004\n",
    "\n",
    "train_hourly_cleaned = hourly_clean_all.loc[mask_2004_h_c]\n",
    "train_daily_cleaned = daily_clean_all.loc[mask_2004_d_c]\n",
    "train_merge_cleaned = merge_clean_all.loc[mask_2004_m_c]\n",
    "\n",
    "train_hourly_cleaned.to_csv(FE_TRAIN_CLEAN_DIR / \"train_2004_fe_hourly_cleaned.csv\")\n",
    "train_daily_cleaned.to_csv(FE_TRAIN_CLEAN_DIR / \"train_2004_fe_daily_cleaned.csv\")\n",
    "train_merge_cleaned.to_csv(FE_TRAIN_CLEAN_DIR / \"train_2004_fe_merge_cleaned.csv\")\n",
    "\n",
    "print(\"\\n[OUTPUT] Cleaned pipeline (train 2004 after anomaly removal)\")\n",
    "print(FE_TRAIN_CLEAN_DIR / \"train_2004_fe_hourly_cleaned.csv\")\n",
    "print(FE_TRAIN_CLEAN_DIR / \"train_2004_fe_daily_cleaned.csv\")\n",
    "print(FE_TRAIN_CLEAN_DIR / \"train_2004_fe_merge_cleaned.csv\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Summary table\n",
    "# --------------------------------------------------\n",
    "summary = pd.DataFrame(\n",
    "    {\n",
    "        \"version\": [\n",
    "            \"train_orig_hourly\",\n",
    "            \"train_orig_daily\",\n",
    "            \"train_orig_merge\",\n",
    "            \"train_cleaned_hourly\",\n",
    "            \"train_cleaned_daily\",\n",
    "            \"train_cleaned_merge\",\n",
    "            \"test_2005_hourly\",\n",
    "            \"test_2005_daily\",\n",
    "            \"test_2005_merge\",\n",
    "        ],\n",
    "        \"n_rows\": [\n",
    "            train_hourly_orig.shape[0],\n",
    "            train_daily_orig.shape[0],\n",
    "            train_merge_orig.shape[0],\n",
    "            train_hourly_cleaned.shape[0],\n",
    "            train_daily_cleaned.shape[0],\n",
    "            train_merge_cleaned.shape[0],\n",
    "            test_hourly.shape[0],\n",
    "            test_daily.shape[0],\n",
    "            test_merge.shape[0],\n",
    "        ],\n",
    "        \"n_features\": [\n",
    "            train_hourly_orig.shape[1],\n",
    "            train_daily_orig.shape[1],\n",
    "            train_merge_orig.shape[1],\n",
    "            train_hourly_cleaned.shape[1],\n",
    "            train_daily_cleaned.shape[1],\n",
    "            train_merge_cleaned.shape[1],\n",
    "            test_hourly.shape[1],\n",
    "            test_daily.shape[1],\n",
    "            test_merge.shape[1],\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "summary_path = FE_DIR / \"fe_train_test_summary.csv\"\n",
    "summary.to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"\\n[SUMMARY]\")\n",
    "print(summary)\n",
    "print(\"\\nSummary saved to:\", summary_path)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7001ca-3b5d-46a9-a4a2-b16e461927e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
